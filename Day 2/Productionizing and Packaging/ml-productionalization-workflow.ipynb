{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Machine Learning Productionalization Workflow\n",
    "\n",
    "- Code and project layout\n",
    "- ML \"Step\" coding style\n",
    "- ML \"Workflow\" coding style\n",
    "- Unit testing style\n",
    "- Python coding style and linting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Workflow\n",
    "\n",
    "The **workflow** is the entire end-to-end machine learning project. It includes \n",
    "\n",
    "- loading the data from data warehouses\n",
    "- feature creation\n",
    "- model training\n",
    "- value prediction\n",
    "- exposure of predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Step and Stage\n",
    "\n",
    "A **step** is the smallest building block of an ML workflow. A step might do one of any number of things:\n",
    "\n",
    "- data loading\n",
    "- data transformation\n",
    "- model training\n",
    "- model inference \n",
    "- model serving\n",
    "\n",
    "A **stage** is a collection of **step**s that all run on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code organization\n",
    "\n",
    "Folder structure\n",
    "\n",
    "- ProjectName/\n",
    "    - packagename/\n",
    "        - stage1/\n",
    "            - steps/\n",
    "                - (e.g.) load_data.py\n",
    "            - tests/\n",
    "            - main.py\n",
    "        - stage2/\n",
    "            - steps/\n",
    "            - tests/\n",
    "            - main.py\n",
    "        - mock_data/\n",
    "        - workflow_test.py\n",
    "        - config.py\n",
    "        - constants.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Code\n",
    "\n",
    "A step is Python module containing a `class` that implements the step. Step classes should provide the following methods:\n",
    "\n",
    "- `__init__` should initialize the step, providing all hyperparameters necessary as arguments to the initializer\n",
    "- `execute(...)` should actually run the step. Execute has as its input any 'upstream' data and may provide output data for 'downstream' consumption as well\n",
    "- (optional) `persist(path)` should persist the results of running the step at the given path\n",
    "\n",
    "Additionally, Step code must provide the following docstrings:\n",
    "\n",
    "- The step module docstring contains an example of how to use the step\n",
    "- The step class docstring contains documentation of\n",
    "    - the `__init__` parameters (configuration of the step)\n",
    "    - expected data in `execute()`\n",
    "    - format of data returned by `execute()` \n",
    "    - persistence strategy, if any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible step is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rick446/src/arborian-classes/data/recommender\n"
     ]
    }
   ],
   "source": [
    "cd data/recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./recommender/stage1/steps/train_test_split.py\n"
     ]
    }
   ],
   "source": [
    "%%file ./recommender/stage1/steps/train_test_split.py\n",
    "\"\"\"\n",
    "Split events into training and test sets\n",
    "\n",
    "Example usage:\n",
    ">>> import pandas as pd\n",
    ">>> from recommender.stage1.train_test_split import TrainTestSplit\n",
    ">>> df = pd.read_csv('workflow_dev/mock_data/events.csv')\n",
    ">>> step = TrainTestSplit(train_percentile=90)\n",
    ">>> (df_train, df_test) = step.execute(df)\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TrainTestSplit():\n",
    "    \"\"\"Workflow step that splits an event stream into two event streams based on a particular\n",
    "    column in the stream.\n",
    "\n",
    "    Init parameters:\n",
    "        column_name: string representing the column to partition based on\n",
    "        train_percentile: percentage of the values in the given column\n",
    "            to be partitioned into the training set\n",
    "\n",
    "    Execution Args:\n",
    "        df: Pandas DataFrame containing an event stream with (at least) a column\n",
    "            with the specified column name\n",
    "\n",
    "    Execution Returns:\n",
    "        Tuple of (training, test) dataframes\n",
    "\n",
    "    Persist Strategy:\n",
    "        Pickle the tuple of dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            column_name='visitorid',\n",
    "            train_percentile=90,\n",
    "    ):\n",
    "        self.column_name = column_name\n",
    "        self.train_percentile = train_percentile\n",
    "\n",
    "    def execute(self, df):\n",
    "        col = df[self.column_name]\n",
    "        split_value = np.percentile(col, self.train_percentile)\n",
    "        train = df[col < split_value]\n",
    "        test = df[col >= split_value]\n",
    "        return train, test\n",
    "\n",
    "    def persist(self, path, data):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(f, data)\n",
    "\n",
    "    def execute_persist(self, path, df):\n",
    "        result = self.execute(df)\n",
    "        self.persist(path, result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the step\n",
    "\n",
    "To test this step, there should be a unittest file defined in the corresponding tests folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  \u001b[34m__pycache__\u001b[m\u001b[m/ main.py      \u001b[34msteps\u001b[m\u001b[m/       \u001b[34mtests\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ./recommender/stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./recommender/stage1/tests/test_train_test_split.py\n"
     ]
    }
   ],
   "source": [
    "%%file ./recommender/stage1/tests/test_train_test_split.py\n",
    "from unittest import TestCase\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from recommender.stage1.steps.train_test_split import TrainTestSplit\n",
    "\n",
    "class TestTestTrainSplit(TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        values = np.r_[:100]\n",
    "        self.df = pd.DataFrame({\n",
    "            'visitorid': values,\n",
    "            'anotherid': values[::-1],\n",
    "        })\n",
    "\n",
    "    def test_execute_defaults(self):\n",
    "        step = TrainTestSplit()\n",
    "        df_train, df_test = step.execute(self.df)\n",
    "        self.assertEqual(len(df_train), 90)\n",
    "        self.assertEqual(df_test.visitorid.min(), 90)\n",
    "\n",
    "    def test_execute_50(self):\n",
    "        step = TrainTestSplit(train_percentile=50)\n",
    "        df_train, df_test = step.execute(self.df)\n",
    "        self.assertEqual(len(df_train), 50)\n",
    "        self.assertEqual(df_test.visitorid.min(), 50)\n",
    "\n",
    "    def test_execute_othercol(self):\n",
    "        step = TrainTestSplit(column_name='anotherid')\n",
    "        df_train, df_test = step.execute(self.df)\n",
    "        self.assertEqual(len(df_train), 90)\n",
    "        self.assertEqual(df_test.visitorid.min(), 0)\n",
    "        self.assertEqual(df_test.anotherid.min(), 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.014s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest recommender/stage1/tests/test_train_test_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow code\n",
    "\n",
    "A workflow is a single Python module named `main.py` in the stage directory containing a `class` that implements a stage of execution. Stage classes should provide the following methods:\n",
    "\n",
    "- `__init__` should initialize the stage and all the steps within the stage. Parameters to initialize the individual steps should be passed as a nested Python dictionary with the keys corresponding to the step names.\n",
    "- `execute(...)` should actually run the stage. \n",
    "- (optional) `persist(path)` should persist the results of running the stage at the given path\n",
    "\n",
    "Additionally, Workflow code must provide the following docstrings:\n",
    "\n",
    "- The step module docstring contains an example of how to use the stage\n",
    "- The stage class docstring contains documentation of\n",
    "    - the `__init__` parameters (configuration of the step)\n",
    "    - any expected data in `execute()`\n",
    "    - format of data returned by `execute()` \n",
    "    - persistence strategy, if any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible workflow stage is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting recommender/stage1/main.py\n"
     ]
    }
   ],
   "source": [
    "%%file recommender/stage1/main.py\n",
    "\"\"\"\n",
    "Load data, extract features, and prepare for fitting.\n",
    "\n",
    "Example usage:\n",
    ">>> from recommender.workflow_dev.workflow_stage1 import Stage1\n",
    ">>> st1 = Stage1(config)\n",
    ">>> st1.execute_persist()\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "from recommender.stage1.steps.data_loader import DataLoader\n",
    "from recommender.stage1.steps.score_events import ScoreEvents\n",
    "from recommender.stage1.steps.group_scores import ScoreGrouper\n",
    "from recommender.stage1.steps.filter_scores import ScoreFilter\n",
    "from recommender.stage1.steps.column_encoder import ColumnEncoder\n",
    "\n",
    "\n",
    "class Stage1():\n",
    "    \"\"\"This stage loads the event and fits the scoring model.\n",
    "\n",
    "    It consists of the following steps:\n",
    "    - load the data\n",
    "    - scoring events\n",
    "    - group scores by visitorid/itemid\n",
    "    - finding relevant visitors\n",
    "    - encoding items for relevant visitors with a LabelEncoder\n",
    "\n",
    "    Example config:\n",
    "    cfg = {\n",
    "        'data_loader': {\n",
    "            'event_filename': 'mock_data/events.csv',\n",
    "        },\n",
    "        'score_events': {\n",
    "            'input_column_name': 'event',\n",
    "            'score_column_name': 'score',\n",
    "            'score_map': {\n",
    "                    'view': 1,\n",
    "                    'addtocart': 5,\n",
    "                    'transaction': 10,\n",
    "            },\n",
    "        },\n",
    "        'encode_itemids': {\n",
    "            'column_name': 'itemid',\n",
    "        },\n",
    "        'group_scores': {\n",
    "            'key': 'visitorid itemid'.split(),\n",
    "        },\n",
    "        'score_filter': {\n",
    "            'min_score': 10,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    Execution Args:\n",
    "        None\n",
    "\n",
    "    Execution Returns:\n",
    "        Python Dictionary with the following keys:\n",
    "            - item_encoder: LabelEncoder() with encoded relevant itemids\n",
    "            - scores: Pandas Series indexed by (visitorid, encoded itemid) with\n",
    "                the score for each relevant visitorid/item\n",
    "        }\n",
    "\n",
    "    Persist strategy:\n",
    "        Pickle the result\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def _load_data(self):\n",
    "        step = DataLoader(**self.config.get('data_loader', {}))\n",
    "        return step.execute()\n",
    "\n",
    "    def _score_events(self, df_events):\n",
    "        step = ScoreEvents(**self.config.get('score_events', {}))\n",
    "        return step.execute(df_events)\n",
    "\n",
    "    def _group_scores(self, df_scored_events):\n",
    "        step = ScoreGrouper(**self.config.get('group_scores', {}))\n",
    "        return step.execute(df_scored_events)\n",
    "\n",
    "    def _filter_scores(self, s_grouped_scores):\n",
    "        step = ScoreFilter(**self.config.get('event_filter', {}))\n",
    "        return step.execute(s_grouped_scores)\n",
    "\n",
    "    def _encode_itemids(self, s_filtered_scores):\n",
    "        step = ColumnEncoder(**self.config.get('encode_itemids', {}))\n",
    "        return step.execute(s_filtered_scores)\n",
    "\n",
    "    def execute(self):\n",
    "        df_events = self._load_data()\n",
    "        df_scored_events = self._score_events(df_events)\n",
    "        s_grouped_scores = self._group_scores(df_scored_events)\n",
    "        df_filtered_scores = self._filter_scores(s_grouped_scores.reset_index())\n",
    "        d_encoder = self._encode_itemids(df_filtered_scores)\n",
    "        return {\n",
    "            'item_encoder': d_encoder['encoder'],\n",
    "            'scores': d_encoder['df'],\n",
    "        }\n",
    "\n",
    "    def persist(self, path, data):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(f, data)\n",
    "\n",
    "    def execute_persist(self, path):\n",
    "        data = self.execute()\n",
    "        self.persist(path, data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the stage\n",
    "\n",
    "Each of the workflow stages should be tested independently in a test script in the package directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting recommender/workflow_test.py\n"
     ]
    }
   ],
   "source": [
    "%%file recommender/workflow_test.py\n",
    "from unittest import TestCase\n",
    "\n",
    "\n",
    "import recommender.constants as C\n",
    "from recommender.stage1.main import Stage1\n",
    "\n",
    "\n",
    "class TestDevWorkflow(TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.config_stage1 = {\n",
    "            'data_loader': {\n",
    "                'event_filename': 'recommender/mock_data/events.csv',\n",
    "            },\n",
    "            'score_events': {\n",
    "                'input_column_name': C.EVENT_COLUMN_NAME,\n",
    "                'score_column_name': C.SCORE_COLUMN_NAME,\n",
    "                'score_map': {\n",
    "                        'view': 1,\n",
    "                        'addtocart': 5,\n",
    "                        'transaction': 10,\n",
    "                },\n",
    "            },\n",
    "            'encode_itemids': {\n",
    "                'column_name': 'itemid',\n",
    "            },\n",
    "            'group_scores': {\n",
    "                'key': 'visitorid itemid'.split(),\n",
    "            },\n",
    "            'score_filter': {\n",
    "                'min_score': 10,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def test_stage_1(self):\n",
    "        stage = Stage1(self.config_stage1)\n",
    "        result = stage.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.044s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest recommender/workflow_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
